import os
import re
import json
import faiss
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import hashlib

# ========== é…ç½® ==========
MD_ROOT = Path("/home/zzm/Project_1/kg-hk/0_mineru_pdf/data_md_final")  # æ›¿æ¢ä¸ºä½ çš„ .md æ–‡ä»¶æ ¹ç›®å½•
VECTOR_DB_DIR = Path("/home/zzm/Project_1/kg-hk/2_kg_construction/kg_vector_db")
VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)

# MODEL_NAME = "/hdd1/checkpoints/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
MODEL_NAME = "/hdd1/checkpoints/sentence-transformers/text2vec-base-chinese"
CHUNK_SIZE = 200  # å­—ç¬¦æ•°
CHUNK_OVERLAP = 50

DEVICE = "cuda"  # æˆ– "cpu"

# ========== åˆå§‹åŒ– ==========
print("ğŸ”„ åŠ è½½è¯­ä¹‰æ¨¡å‹...")
model = SentenceTransformer(MODEL_NAME, device=DEVICE)
embedding_dim = model.get_sentence_embedding_dimension()

# ========== å·¥å…·å‡½æ•° ==========

def split_text_by_fixed_size(
    text: str,
    chunk_size: int = 200,
    overlap: int = 50,
    separator: str = "\n"
) -> List[str]:
    """
    æŒ‰å›ºå®šå­—ç¬¦æ•°åˆ†å‰²æ–‡æœ¬ï¼Œå¸¦æœ‰é‡å éƒ¨åˆ†
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        chunk_size: æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
        overlap: å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
        separator: ç”¨äºæŸ¥æ‰¾è‡ªç„¶è¾¹ç•Œçš„åˆ†éš”ç¬¦
    
    Returns:
        åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
    """
    if not text:
        return []
    
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        end = start + chunk_size
        
        if end >= text_length:
            # æœ€åä¸€å—
            chunk = text[start:].strip()
            if chunk:
                chunks.append(chunk)
            break
        
        # å°è¯•åœ¨è‡ªç„¶è¾¹ç•Œå¤„æ–­å¼€ï¼ˆå¥å­ã€æ®µè½ç­‰ï¼‰
        # å…ˆæ‰¾å¥å·ã€é—®å·ã€æ„Ÿå¹å·
        boundary_chars = ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ', '\n', ';', 'ï¼›', ' ', 'ã€€']
        
        # ä»endä½ç½®å¾€å‰æ‰¾æœ€è¿‘çš„åˆ†éš”ç¬¦
        boundary_found = False
        for i in range(end, max(start + chunk_size // 2, start), -1):
            if i < len(text) and text[i] in boundary_chars:
                end = i + 1  # åŒ…å«åˆ†éš”ç¬¦
                boundary_found = True
                break
        
        # å¦‚æœæ²¡æ‰¾åˆ°åˆ†éš”ç¬¦ï¼Œå°±å¼ºåˆ¶åœ¨å•è¯è¾¹ç•Œå¤„æ–­å¼€
        if not boundary_found:
            # æ‰¾ç©ºæ ¼æˆ–æ ‡ç‚¹
            for i in range(end, start + chunk_size // 2, -1):
                if i < len(text) and text[i] in [' ', ',', 'ï¼Œ', 'ã€']:
                    end = i
                    boundary_found = True
                    break
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        # ç§»åŠ¨èµ·å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å 
        start = end - overlap
        if start < 0:
            start = 0
    
    return chunks

def chunk_markdown_fixed_size(
    text: str,
    chunk_size: int = 200,
    overlap: int = 50
) -> List[Dict[str, str]]:
    """
    æŒ‰å›ºå®šå­—ç¬¦æ•°åˆ†å‰²Markdownæ–‡æœ¬
    
    Returns:
        List[Dict[str, str]]: æ¯ä¸ªå—çš„ä¿¡æ¯
    """
    if not text:
        return []
    
    # æ¸…ç†æ–‡æœ¬ï¼šç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
    text = re.sub(r'\n\s*\n', '\n\n', text)  # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
    text = text.strip()
    
    # åˆ†å‰²æ–‡æœ¬
    chunks = split_text_by_fixed_size(text, chunk_size, overlap)
    
    # æ„å»ºç»“æœ
    result = []
    for i, chunk_text in enumerate(chunks):
        # ä¸ºæ¯ä¸ªå—æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_parts = []
        
        # æ·»åŠ ä¸Šä¸€å—çš„éƒ¨åˆ†å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
        if i > 0 and len(chunks[i-1]) > 50:
            prev_context = chunks[i-1][-50:]
            context_parts.append(f"[å‰æ–‡] {prev_context}")
        
        context_parts.append(chunk_text)
        
        # æ·»åŠ ä¸‹ä¸€å—çš„éƒ¨åˆ†å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
        if i < len(chunks) - 1 and len(chunks[i+1]) > 50:
            next_context = chunks[i+1][:50]
            context_parts.append(f"[åæ–‡] {next_context}")
        
        # å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆç”¨äºå‘é‡åŒ–ï¼‰
        full_context = "\n".join(context_parts)
        
        result.append({
            "text": chunk_text,  # åŸå§‹å—æ–‡æœ¬
            "full_context": full_context,  # å¸¦ä¸Šä¸‹æ–‡çš„å®Œæ•´æ–‡æœ¬
            "chunk_index": i,
            "total_chunks": len(chunks)
        })
    
    return result

def extract_md_metadata(text: str, file_path: Path) -> Dict:
    """
    ä»Markdownæ–‡æœ¬ä¸­æå–å…ƒæ•°æ®
    
    Args:
        text: Markdownæ–‡æœ¬
        file_path: æ–‡ä»¶è·¯å¾„
    
    Returns:
        å…ƒæ•°æ®å­—å…¸
    """
    metadata = {
        "file_name": file_path.name,
        "file_path": str(file_path.resolve()),
        "file_hash": hashlib.md5(text.encode()).hexdigest()[:16],
        "total_chars": len(text),
        "lines": len(text.splitlines()),
        "extracted_title": "",
        "extracted_headings": []
    }
    
    # æå–æ ‡é¢˜
    title_patterns = [
        (r'^#\s+(.+)$', 1),  # ä¸€çº§æ ‡é¢˜
        (r'^title:\s*(.+)$', 1),  # YAML front matter title
        (r'^# (.+)$', 1),  # å¦ä¸€ç§ä¸€çº§æ ‡é¢˜æ ¼å¼
    ]
    
    for pattern, group_idx in title_patterns:
        match = re.search(pattern, text, re.MULTILINE)
        if match:
            metadata["extracted_title"] = match.group(group_idx).strip()
            break
    
    # æå–æ‰€æœ‰æ ‡é¢˜
    heading_pattern = r'^(#{1,6})\s+(.+)$'
    headings = re.findall(heading_pattern, text, re.MULTILINE)
    metadata["extracted_headings"] = [f"{'#' * len(h[0])} {h[1].strip()}" for h in headings[:10]]  # æœ€å¤šå–å‰10ä¸ª
    
    # æå–æ–‡æ¡£å¼€å¤´éƒ¨åˆ†ä½œä¸ºæ‘˜è¦
    first_lines = text.split('\n')[:10]
    summary = ' '.join([line.strip() for line in first_lines if line.strip()])[:200]
    metadata["summary"] = summary + "..." if len(summary) >= 200 else summary
    
    return metadata

def load_and_chunk_md_files(md_root: Path) -> Tuple[List[Dict], List[Dict]]:
    """
    åŠ è½½å¹¶åˆ†å—æ‰€æœ‰MDæ–‡ä»¶
    
    Returns:
        Tuple[List[Dict], List[Dict]]: (å—åˆ—è¡¨, æ–‡ä»¶å…ƒæ•°æ®åˆ—è¡¨)
    """
    chunks_with_meta = []
    file_metadata_list = []
    
    # æŸ¥æ‰¾æ‰€æœ‰MDæ–‡ä»¶
    md_files = list(md_root.rglob("*.md"))
    print(f"ğŸ“‚ æ‰¾åˆ° {len(md_files)} ä¸ª .md æ–‡ä»¶ï¼Œå¼€å§‹åˆ†å—...")
    
    for md_file in tqdm(md_files, desc="åˆ†å—å¤„ç†"):
        try:
            # è¯»å–æ–‡ä»¶
            text = md_file.read_text(encoding='utf-8', errors='ignore')
            
            # æå–æ–‡ä»¶å…ƒæ•°æ®
            file_metadata = extract_md_metadata(text, md_file)
            file_metadata_list.append(file_metadata)
            
            # åˆ†å—
            chunked = chunk_markdown_fixed_size(
                text,
                chunk_size=CHUNK_SIZE,
                overlap=CHUNK_OVERLAP
            )
            
            # ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®
            for i, item in enumerate(chunked):
                if not item["text"].strip():
                    continue
                    
                chunk_metadata = {
                    "file_name": md_file.name,
                    "file_path": str(md_file.resolve()),
                    "file_hash": file_metadata["file_hash"],
                    "chunk_index": i,
                    "total_chunks": len(chunked),
                    "chunk_size": len(item["text"]),
                    "context_size": len(item["full_context"]),
                    "title": file_metadata["extracted_title"],
                    "summary": file_metadata["summary"][:100]  # åªä¿ç•™å‰100å­—ç¬¦
                }
                
                chunks_with_meta.append({
                    "text": item["full_context"],  # ä½¿ç”¨å¸¦ä¸Šä¸‹æ–‡çš„æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–
                    "original_text": item["text"],  # åŸå§‹å—æ–‡æœ¬
                    "metadata": chunk_metadata
                })
                
        except Exception as e:
            print(f"âš ï¸ å¤„ç†æ–‡ä»¶ {md_file.name} å¤±è´¥: {e}")
            continue
    
    return chunks_with_meta, file_metadata_list

def build_vector_db(chunks_with_meta: List[Dict], output_dir: Path):
    """
    æ„å»ºå‘é‡æ•°æ®åº“
    
    Args:
        chunks_with_meta: å¸¦å…ƒæ•°æ®çš„æ–‡æœ¬å—åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
    """
    if not chunks_with_meta:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ–‡æœ¬å—ï¼Œè·³è¿‡å‘é‡æ•°æ®åº“æ„å»º")
        return
    
    # å‡†å¤‡æ–‡æœ¬
    texts = [item["text"] for item in chunks_with_meta]
    print(f"ğŸ§  æ­£åœ¨å¯¹ {len(texts)} ä¸ªæ–‡æœ¬å—è¿›è¡Œå‘é‡åŒ–...")
    
    try:
        # æ‰¹é‡ç¼–ç 
        embeddings = model.encode(
            texts,
            batch_size=128,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True  # å½’ä¸€åŒ–ä»¥ä¾¿ä½¿ç”¨å†…ç§¯
        )
        
        print(f"âœ… å‘é‡åŒ–å®Œæˆï¼Œç»´åº¦: {embeddings.shape}")
        
        # ä¿å­˜å‘é‡
        embeddings_path = output_dir / "embeddings.npy"
        np.save(embeddings_path, embeddings)
        print(f"ğŸ’¾ å‘é‡å·²ä¿å­˜: {embeddings_path}")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_path = output_dir / "metadata.jsonl"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            for item in chunks_with_meta:
                # åªä¿å­˜metadataéƒ¨åˆ†ï¼Œå‡å°æ–‡ä»¶å¤§å°
                f.write(json.dumps(item["metadata"], ensure_ascii=False) + "\n")
        print(f"ğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
        
        # ä¿å­˜å®Œæ•´çš„å—ä¿¡æ¯ï¼ˆåŒ…å«åŸå§‹æ–‡æœ¬ï¼‰
        chunks_path = output_dir / "chunks.json"
        chunks_data = []
        for item in chunks_with_meta:
            chunks_data.append({
                "metadata": item["metadata"],
                "original_text": item["original_text"],
                "context_text": item["text"]
            })
        
        with open(chunks_path, 'w', encoding='utf-8') as f:
            json.dump(chunks_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ å®Œæ•´å—æ•°æ®å·²ä¿å­˜: {chunks_path}")
        
        # æ„å»ºFAISSç´¢å¼•
        print("ğŸ”§ æ„å»ºFAISSç´¢å¼•...")
        index = faiss.IndexFlatIP(embedding_dim)  # å†…ç§¯ç´¢å¼•ï¼ˆå‘é‡å·²å½’ä¸€åŒ–ï¼‰
        index.add(embeddings.astype(np.float32))
        
        # ä¿å­˜ç´¢å¼•
        faiss_path = output_dir / "faiss.index"
        faiss.write_index(index, str(faiss_path))
        print(f"ğŸ’¾ FAISSç´¢å¼•å·²ä¿å­˜: {faiss_path}")
        
        # ä¿å­˜é…ç½®ä¿¡æ¯
        config = {
            "model_name": MODEL_NAME,
            "embedding_dim": embedding_dim,
            "chunk_size": CHUNK_SIZE,
            "chunk_overlap": CHUNK_OVERLAP,
            "total_chunks": len(chunks_with_meta),
            "total_files": len(set([item["metadata"]["file_hash"] for item in chunks_with_meta])),
            "build_time": np.datetime64('now').astype(str)
        }
        
        config_path = output_dir / "config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜: {config_path}")
        
        print(f"âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
        print(f"   æ€»å—æ•°: {len(chunks_with_meta)}")
        print(f"   å‘é‡ç»´åº¦: {embedding_dim}")
        print(f"   è¾“å‡ºç›®å½•: {output_dir}")
        
    except Exception as e:
        print(f"âŒ æ„å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}")
        raise

def build_statistics(chunks_with_meta: List[Dict], file_metadata_list: List[Dict], output_dir: Path):
    """
    æ„å»ºç»Ÿè®¡ä¿¡æ¯
    
    Args:
        chunks_with_meta: å—åˆ—è¡¨
        file_metadata_list: æ–‡ä»¶å…ƒæ•°æ®åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
    """
    if not chunks_with_meta:
        return
    
    # åŸºæœ¬ç»Ÿè®¡
    total_chunks = len(chunks_with_meta)
    total_files = len(file_metadata_list)
    
    # å—å¤§å°ç»Ÿè®¡
    chunk_sizes = [len(item["original_text"]) for item in chunks_with_meta]
    avg_chunk_size = np.mean(chunk_sizes) if chunk_sizes else 0
    max_chunk_size = max(chunk_sizes) if chunk_sizes else 0
    min_chunk_size = min(chunk_sizes) if chunk_sizes else 0
    
    # æ–‡ä»¶å¤§å°ç»Ÿè®¡
    file_chars = [meta["total_chars"] for meta in file_metadata_list]
    avg_file_size = np.mean(file_chars) if file_chars else 0
    
    # æ¯ä¸ªæ–‡ä»¶çš„å—æ•°ç»Ÿè®¡
    file_chunk_counts = {}
    for item in chunks_with_meta:
        file_hash = item["metadata"]["file_hash"]
        file_chunk_counts[file_hash] = file_chunk_counts.get(file_hash, 0) + 1
    
    avg_chunks_per_file = np.mean(list(file_chunk_counts.values())) if file_chunk_counts else 0
    
    # æ„å»ºç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total_files": total_files,
        "total_chunks": total_chunks,
        "chunk_size_stats": {
            "average": float(avg_chunk_size),
            "maximum": int(max_chunk_size),
            "minimum": int(min_chunk_size),
            "target_size": CHUNK_SIZE
        },
        "file_size_stats": {
            "average_chars": float(avg_file_size),
            "total_files": total_files
        },
        "chunk_distribution": {
            "average_per_file": float(avg_chunks_per_file),
            "files_with_chunks": len(file_chunk_counts)
        },
        "processing_summary": {
            "chunk_size": CHUNK_SIZE,
            "chunk_overlap": CHUNK_OVERLAP,
            "model_used": MODEL_NAME
        }
    }
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_path = output_dir / "statistics.json"
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"   æ€»å—æ•°: {total_chunks}")
    print(f"   å¹³å‡å—å¤§å°: {avg_chunk_size:.1f} å­—ç¬¦")
    print(f"   æœ€å¤§å—å¤§å°: {max_chunk_size} å­—ç¬¦")
    print(f"   æœ€å°å—å¤§å°: {min_chunk_size} å­—ç¬¦")
    print(f"   å¹³å‡æ¯ä¸ªæ–‡ä»¶å—æ•°: {avg_chunks_per_file:.1f}")

class VectorDatabase:
    """å‘é‡æ•°æ®åº“æŸ¥è¯¢ç±»"""
    
    def __init__(self, db_dir: Path):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        self.db_dir = db_dir
        
        # åŠ è½½ç´¢å¼•
        index_path = db_dir / "faiss.index"
        if not index_path.exists():
            raise FileNotFoundError(f"FAISSç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_path}")
        
        self.index = faiss.read_index(str(index_path))
        
        # åŠ è½½å…ƒæ•°æ®
        metadata_path = db_dir / "metadata.jsonl"
        if not metadata_path.exists():
            raise FileNotFoundError(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_path}")
        
        self.metadata = []
        with open(metadata_path, 'r', encoding='utf-8') as f:
            for line in f:
                self.metadata.append(json.loads(line.strip()))
        
        # åŠ è½½é…ç½®
        config_path = db_dir / "config.json"
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
        else:
            self.config = {}
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        æœç´¢ç›¸ä¼¼æ–‡æœ¬
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
        
        Returns:
            ç›¸ä¼¼æ–‡æœ¬åˆ—è¡¨
        """
        # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        query_embedding = model.encode([query], normalize_embeddings=True)
        
        # æœç´¢
        distances, indices = self.index.search(query_embedding.astype(np.float32), top_k)
        
        # æ„å»ºç»“æœ
        results = []
        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
            if idx < len(self.metadata):
                metadata = self.metadata[idx]
                results.append({
                    "rank": i + 1,
                    "score": float(dist),
                    "metadata": metadata,
                    "chunk_index": metadata.get("chunk_index", 0),
                    "file_name": metadata.get("file_name", ""),
                    "title": metadata.get("title", "")
                })
        
        return results

# ========== ä¸»ç¨‹åº ==========
if __name__ == "__main__":
    print("=" * 60)
    print("Markdownæ–‡æœ¬å‘é‡æ•°æ®åº“æ„å»ºå·¥å…·")
    print("=" * 60)
    print(f"è¾“å…¥ç›®å½•: {MD_ROOT}")
    print(f"è¾“å‡ºç›®å½•: {VECTOR_DB_DIR}")
    print(f"å—å¤§å°: {CHUNK_SIZE} å­—ç¬¦")
    print(f"é‡å å¤§å°: {CHUNK_OVERLAP} å­—ç¬¦")
    print(f"æ¨¡å‹: {MODEL_NAME}")
    print("-" * 60)
    
    # 1. åŠ è½½å¹¶åˆ†å—
    print("\nğŸ“„ æ­¥éª¤1: åŠ è½½å¹¶åˆ†å—Markdownæ–‡ä»¶...")
    chunks_with_meta, file_metadata_list = load_and_chunk_md_files(MD_ROOT)
    
    if not chunks_with_meta:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡æœ¬å—ï¼Œé€€å‡ºã€‚")
        exit(1)
    
    print(f"âœ… å…±ç”Ÿæˆ {len(chunks_with_meta)} ä¸ªæ–‡æœ¬å—ã€‚")
    
    # 2. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š æ­¥éª¤2: ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...")
    build_statistics(chunks_with_meta, file_metadata_list, VECTOR_DB_DIR)
    
    # 3. æ„å»ºå‘é‡æ•°æ®åº“
    print("\nğŸ”§ æ­¥éª¤3: æ„å»ºå‘é‡æ•°æ®åº“...")
    build_vector_db(chunks_with_meta, VECTOR_DB_DIR)
    
    # 4. æµ‹è¯•æŸ¥è¯¢
    print("\nğŸ” æ­¥éª¤4: æµ‹è¯•å‘é‡æ•°æ®åº“æŸ¥è¯¢...")
    try:
        db = VectorDatabase(VECTOR_DB_DIR)
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "ç©ºæ°”åŠ¨åŠ›å­¦æ ‡å‡†",
            "ææ–™è§„èŒƒ",
            "æµ‹è¯•æ–¹æ³•"
        ]
        
        print("\nğŸ§ª æµ‹è¯•æŸ¥è¯¢ç»“æœ:")
        for query in test_queries:
            print(f"\næŸ¥è¯¢: '{query}'")
            results = db.search(query, top_k=3)
            
            for result in results:
                print(f"  å¾—åˆ†: {result['score']:.4f} - æ–‡ä»¶: {result['file_name']}")
                if result['title']:
                    print(f"      æ ‡é¢˜: {result['title']}")
    
    except Exception as e:
        print(f"âš ï¸ æµ‹è¯•æŸ¥è¯¢å¤±è´¥: {e}")
    
    print("\nğŸ‰ å‘é‡æ•°æ®åº“æ„å»ºå®Œæ¯•ï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {VECTOR_DB_DIR}")
    print(f"ğŸ“„ ä¸»è¦æ–‡ä»¶:")
    print(f"  - faiss.index: FAISSç´¢å¼•æ–‡ä»¶")
    print(f"  - embeddings.npy: å‘é‡æ•°æ®")
    print(f"  - metadata.jsonl: å…ƒæ•°æ®")
    print(f"  - chunks.json: å®Œæ•´å—æ•°æ®")
    print(f"  - config.json: é…ç½®ä¿¡æ¯")
    print(f"  - statistics.json: ç»Ÿè®¡ä¿¡æ¯")