import json
from pathlib import Path
from openai import OpenAI
import re
from tqdm import tqdm
import math

# ========== é…ç½® vLLM ==========
VLLM_BASE_URL = "URL"
VLLM_API_KEY = "EMPTY"
MODEL_NAME = "MODEL_NAME"

client = OpenAI(base_url=VLLM_BASE_URL, api_key=VLLM_API_KEY)

# ========== è·¯å¾„é…ç½® ==========
INPUT_ROOT = Path(r"/home/zzm/Project_1/kg-hk/1_extract_data/empty")
OUTPUT_ROOT = Path(r"/home/zzm/Project_1/kg-hk/1_extract_data/kg_data")
OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)

# ========== å¤„ç†å‚æ•°é…ç½® ==========
CHUNK_SIZE = 1500  # æ¯å—5000å­—ç¬¦ # 6000
CHUNK_OVERLAP = 100  # é‡å 100å­—ç¬¦
START_FILE_INDEX = 16  # ä»ç¬¬å‡ ä¸ªæ–‡ä»¶å¼€å§‹å¤„ç†ï¼ˆä»1å¼€å§‹è®¡æ•°ï¼‰
END_FILE_INDEX = 35   # åˆ°ç¬¬å‡ ä¸ªæ–‡ä»¶ç»“æŸï¼ˆåŒ…å«ï¼‰

def split_text_into_chunks(text: str, chunk_size: int = 5000, overlap: int = 100) -> list:
    """å°†æ–‡æœ¬åˆ†å‰²æˆå›ºå®šå¤§å°çš„å—ï¼Œå¸¦æœ‰é‡å éƒ¨åˆ†"""
    chunks = []
    start = 0
    
    while start < len(text):
        # è®¡ç®—å—çš„ç»“æŸä½ç½®
        end = start + chunk_size
        
        # å¦‚æœå—ç»“æŸåœ¨å¥å­ä¸­é—´ï¼Œå°½é‡åœ¨å¥å·ã€åˆ†å·æˆ–æ¢è¡Œå¤„æ–­å¼€
        if end < len(text):
            # å¯»æ‰¾åˆé€‚çš„æ–­ç‚¹
            for break_char in ['\n', 'ã€‚', 'ï¼›', '. ', '; ']:
                break_pos = text.rfind(break_char, start, end)
                if break_pos != -1 and break_pos > start + chunk_size // 2:
                    end = break_pos + 1
                    break
        
        # è·å–å½“å‰å—
        chunk = text[start:end]
        chunks.append(chunk)
        
        # ç§»åŠ¨èµ·å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å 
        start = end - overlap
        
        # é˜²æ­¢æ— é™å¾ªç¯
        if start <= 0:
            break
            
    return chunks

def extract_standard_id(md_filename: str):
    """ä»æ–‡ä»¶åæå–æ ‡å‡†ID"""
    return Path(md_filename).stem.replace("_", " ").replace("+", " ")

# æ›´æ–°åçš„promptï¼Œè¦æ±‚è¾“å‡ºJSONæ•°ç»„æ ¼å¼
build_prompt = f"""
ä½ æ˜¯èˆªç©ºèˆªå¤©åˆ¶é€ çŸ¥è¯†æŠ½å–ä¸“å®¶ã€‚è¯·ä»markdownæ ‡å‡†æ–‡æ¡£ä¸­æŠ½å–ç»“æ„åŒ–ä¸‰å…ƒç»„çŸ¥è¯†ã€‚

ã€ä»»åŠ¡ã€‘
1. æ–‡æ¡£å…ƒä¿¡æ¯ï¼šæ ‡å‡†ç¼–å·ã€æ ‡é¢˜ã€å½’å£/èµ·è‰å•ä½
2. æŠ€æœ¯å®ä½“å…³ç³»ï¼šéƒ¨ä»¶ã€ææ–™ã€å·¥è‰ºã€å‚æ•°ç­‰å…³ç³»

ã€å®ä½“ç±»å‹ã€‘(typeå­—æ®µæ ‡æ³¨):
- Standard(æ ‡å‡†æ–‡æ¡£), Title(æ ‡é¢˜), Component(ç»“æ„éƒ¨ä»¶), Material(ææ–™), Process(å·¥è‰º)
- Equipment(è®¾å¤‡), Parameter(å‚æ•°), Value(å€¼), Organization(æœºæ„)
- Defect(ç¼ºé™·), Requirement(è¦æ±‚), Test(è¯•éªŒ)

ã€å…³ç³»ç±»å‹ã€‘:
- å±‚çº§: part_of(Aæ˜¯Béƒ¨åˆ†), is_a(Aæ˜¯Bç±»å‹)
- å±æ€§: has_parameter(Aæœ‰å‚æ•°B), parameter_value(å‚æ•°Aå€¼ä¸ºB)
- çº¦æŸ: must_follow(Aéµå¾ªB), reference_to(Aå‚è€ƒB), applicable_to(Aé€‚ç”¨äºB)
- å› æœ: cause(Aå¯¼è‡´B), prevent(Aé˜²æ­¢B)
- æ—¶åº: precede(Aåœ¨Bå‰), follow(Aåœ¨Bå)
- éªŒè¯: verify_by(Aé€šè¿‡BéªŒè¯), test_method(Aæµ‹è¯•æ–¹æ³•ä¸ºB)
- æ–‡æ¡£: title(Aæ ‡é¢˜ä¸ºB), issued_by(Aç”±Bå‘å¸ƒ), drafted_by(Aç”±Bèµ·è‰), replace(Aæ›¿ä»£B), reference(Aå¼•ç”¨B)

ã€è§„åˆ™ã€‘:
1. ä¸»å®ä½“ç¡®å®š: æ ‡å‡†ç¼–å·æˆ–æ–‡æ¡£æ ‡é¢˜
2. ä»£è¯å¤„ç†: "æœ¬æ–‡ä»¶"/"æœ¬æ ‡å‡†"ç­‰ä»£è¯æ›¿æ¢ä¸ºä¸»å®ä½“
3. é€å¥åˆ†æ: æ¯å¥è¯ç‹¬ç«‹æŠ½å–
4. è¡¨æ ¼å…¬å¼: æŠ½å–å…³é”®ä¿¡æ¯
5. å…³ç³»å¼ºåº¦: æ ¹æ®"å¿…é¡»"/"åº”"/"å®œ"/"å¯"åˆ¤æ–­

ã€è¾“å‡ºæ ¼å¼ã€‘JSONæ•°ç»„:
[
  {{
    "head": {{"name": "å®ä½“å", "type": "å®ä½“ç±»å‹"}},
    "relation": "å…³ç³»ç±»å‹",
    "tail": {{"name": "å®ä½“å", "type": "å®ä½“ç±»å‹"}},
    "paragraph": "åŸæ–‡å¥å­",
    "source": "",
    "confidence": 0.0-1.0
  }}
]

ã€ç¤ºä¾‹ã€‘:
è¾“å…¥: "HB 8768-2025ã€Šæ°‘ç”¨é£æœºå¤åˆææ–™é›·è¾¾ç½©ä¿®ç†é€šç”¨è¦æ±‚ã€‹å‘å¸ƒã€‚æœ¬æ ‡å‡†è§„å®šäº†å¤åˆææ–™é›·è¾¾ç½©ä¿®ç†è¦æ±‚ã€‚å›ºåŒ–å‡æ¸©é€Ÿç‡ä¸å¾—è¶…è¿‡1.5Â°C/minã€‚"

è¾“å‡º:
[
  {{
    "head": {{"name": "HB 8768-2025", "type": "Standard"}},
    "relation": "title",
    "tail": {{"name": "æ°‘ç”¨é£æœºå¤åˆææ–™é›·è¾¾ç½©ä¿®ç†é€šç”¨è¦æ±‚", "type": "Title"}},
    "paragraph": "HB 8768-2025ã€Šæ°‘ç”¨é£æœºå¤åˆææ–™é›·è¾¾ç½©ä¿®ç†é€šç”¨è¦æ±‚ã€‹å‘å¸ƒã€‚",
    "source": "",
    "confidence": 1.0
  }},
  {{
    "head": {{"name": "HB 8768-2025", "type": "Standard"}},
    "relation": "applicable_to",
    "tail": {{"name": "å¤åˆææ–™é›·è¾¾ç½©", "type": "Component"}},
    "paragraph": "æœ¬æ ‡å‡†è§„å®šäº†å¤åˆææ–™é›·è¾¾ç½©ä¿®ç†è¦æ±‚ã€‚",
    "source": "",
    "confidence": 0.9
  }}
]

ã€é‡è¦ã€‘:
1. åªè¾“å‡ºJSONæ•°ç»„
2. æ— ä¸‰å…ƒç»„æ—¶è¾“å‡º: []
3. sourceç•™ç©º
4. ç½®ä¿¡åº¦åŸºäºå…³ç³»æ˜ç¡®æ€§

è¯·ä»ä»¥ä¸‹æ–‡æ¡£æŠ½å–ä¸‰å…ƒç»„:
/no_think
"""

def call_vllm(text_chunk: str, md_filename: str):
    """è°ƒç”¨vLLM APIå¤„ç†æ–‡æœ¬å—"""
    try:
        # å‡†å¤‡å®Œæ•´æç¤º
        full_prompt = build_prompt + "\n\næ–‡æ¡£å—å†…å®¹ï¼š\n" + text_chunk
        
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç»“æ„åŒ–ä¿¡æ¯æŠ½å–ä¸“å®¶ã€‚"},
                {"role": "user", "content": full_prompt}
            ],
            temperature=0.0,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šè¾“å‡º
            max_tokens=12000
        )
        
        raw_output = response.choices[0].message.content.strip()
        
        # æ¸…ç†è¾“å‡ºï¼Œç§»é™¤å¯èƒ½çš„å¤šä½™æ–‡æœ¬
        cleaned = re.sub(r'(\<think\>.*?\<\/think\>)', '', raw_output, flags=re.DOTALL | re.IGNORECASE)
        cleaned = cleaned.strip()
        print(f"ğŸ“ æ¨¡å‹è¾“å‡ºç‰‡æ®µ: {cleaned}...")
        
        # æŸ¥æ‰¾JSONæ•°ç»„çš„å¼€å§‹å’Œç»“æŸ
        start = cleaned.find('[')
        end = cleaned.rfind(']')
        
        if start == -1 or end == -1 or start >= end:
            print(f"âš ï¸  æ— æ³•åœ¨è¾“å‡ºä¸­æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ•°ç»„ï¼Œè¾“å‡ºé•¿åº¦ï¼š{len(cleaned)}")
            return []
        
        json_str = cleaned[start:end+1]
        
        # å°è¯•è§£æJSON
        try:
            triples = json.loads(json_str)
            if not isinstance(triples, list):
                print(f"âš ï¸  è§£æç»“æœä¸æ˜¯åˆ—è¡¨ï¼Œç±»å‹ï¼š{type(triples)}")
                return []
            
            # å¤„ç†æ¯ä¸ªä¸‰å…ƒç»„ï¼Œæ·»åŠ sourceå­—æ®µ
            processed_triples = []
            for triple in triples:
                if isinstance(triple, dict):
                    # ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µéƒ½å­˜åœ¨
                    processed_triple = {
                        "head": triple.get("head", {"name": "", "type": ""}),
                        "relation": triple.get("relation", ""),
                        "tail": triple.get("tail", {"name": "", "type": ""}),
                        "paragraph": triple.get("paragraph", ""),
                        "source": md_filename,  # æ·»åŠ æ–‡æ¡£åç§°
                        "confidence": triple.get("confidence", 0.5)
                    }
                    processed_triples.append(processed_triple)
            
            return processed_triples
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æé”™è¯¯: {e}")
            print(f"JSONå­—ç¬¦ä¸²ç‰‡æ®µ: {json_str[:200]}...")
            return []
            
    except Exception as e:
        print(f"âš ï¸  è°ƒç”¨vLLM APIæ—¶å‡ºé”™: {e}")
        return []

def process_single_file(md_path: Path, output_path: Path):
    """å¤„ç†å•ä¸ªæ–‡ä»¶"""
    try:
        with open(md_path, 'r', encoding='utf-8') as f:
            text = f.read()
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {md_path} - {e}")
        return 0
    
    if not text.strip():
        print(f"âš ï¸ è·³è¿‡ç©ºæ–‡ä»¶: {md_path.name}")
        return 0
    
    # åˆ†å—å¤„ç†
    chunks = split_text_into_chunks(text, CHUNK_SIZE, CHUNK_OVERLAP)
    print(f"ğŸ“„ æ–‡ä»¶ '{md_path.name}' åˆ†å‰²ä¸º {len(chunks)} å—ï¼Œæ¯å—ä¸º{len(text)/len(chunks)}å­—ç¬¦")
    
    all_triples = []
    
    # å¤„ç†æ¯ä¸ªå—
    for i, chunk in enumerate(tqdm(chunks, desc=f"å¤„ç† {md_path.name}", leave=False)):
        # å¤„ç†è½¬ä¹‰å­—ç¬¦
        cleaned_chunk = chunk.replace('\\', '\\\\')
        
        # è°ƒç”¨æ¨¡å‹
        triples = call_vllm(cleaned_chunk, md_path.name)
        
        # æ·»åŠ å½“å‰å—çš„ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
        for triple in triples:
            triple["chunk_index"] = i
            triple["chunk_size"] = len(chunk)
        
        all_triples.extend(triples)
    
    # å»é‡ï¼šåŸºäºheadã€relationã€tailå’Œparagraphå»é‡
    seen = set()
    unique_triples = []
    
    for triple in all_triples:
        # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦
        head_name = triple.get("head", {}).get("name", "") if isinstance(triple.get("head"), dict) else str(triple.get("head", ""))
        tail_name = triple.get("tail", {}).get("name", "") if isinstance(triple.get("tail"), dict) else str(triple.get("tail", ""))
        key = (head_name, 
               triple.get("relation", ""), 
               tail_name,
               triple.get("paragraph", ""))
        
        if key not in seen:
            seen.add(key)
            unique_triples.append(triple)
    
    # ä¿å­˜ç»“æœ
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶ï¼ˆæ¨èä½¿ç”¨JSONï¼Œå› ä¸ºç»“æ„ç»Ÿä¸€ï¼‰
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(unique_triples, f, ensure_ascii=False, indent=2)
    
    return len(unique_triples)

def main():
    """ä¸»å‡½æ•°"""
    # æ”¶é›†æ‰€æœ‰ .md æ–‡ä»¶
    md_files_info = []
    
    # è·å–æ‰€æœ‰å­æ–‡ä»¶å¤¹
    subfolders = [f for f in INPUT_ROOT.iterdir() if f.is_dir()]
    
    if not subfolders:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å­æ–‡ä»¶å¤¹ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾mdæ–‡ä»¶...")
        # å¦‚æœæ²¡æœ‰å­æ–‡ä»¶å¤¹ï¼Œç›´æ¥åœ¨æ ¹ç›®å½•æŸ¥æ‰¾
        md_files = list(INPUT_ROOT.glob("*.md"))
        for md_file in md_files:
            out_file = OUTPUT_ROOT / (md_file.stem + ".json")
            md_files_info.append((md_file, out_file))
    else:
        # éå†æ‰€æœ‰å­æ–‡ä»¶å¤¹
        for folder in subfolders:
            md_files = list(folder.glob("*.md"))
            for md_file in md_files:
                # ä¿æŒæ–‡ä»¶å¤¹ç»“æ„
                relative_path = md_file.relative_to(INPUT_ROOT)
                out_file = OUTPUT_ROOT / relative_path.with_suffix('.json')
                md_files_info.append((md_file, out_file))
    
    if not md_files_info:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• .md æ–‡ä»¶ï¼")
        return
    
    # æŒ‰æ–‡ä»¶åæ’åºï¼Œç¡®ä¿å¤„ç†é¡ºåºä¸€è‡´
    md_files_info.sort(key=lambda x: str(x[0]))
    
    total_files = len(md_files_info)
    print(f"ğŸ“ å…±å‘ç° {total_files} ä¸ª .md æ–‡ä»¶")
    
    # åº”ç”¨æ–‡ä»¶èŒƒå›´é™åˆ¶ï¼ˆè½¬æ¢ä¸º0-basedç´¢å¼•ï¼‰
    start_idx = max(0, START_FILE_INDEX - 1)
    end_idx = min(total_files, END_FILE_INDEX)
    
    files_to_process = md_files_info[start_idx:end_idx]
    
    print(f"ğŸ”§ å°†å¤„ç†ä»ç¬¬{START_FILE_INDEX}åˆ°ç¬¬{END_FILE_INDEX}ä¸ªæ–‡ä»¶ï¼Œå…± {len(files_to_process)} ä¸ªæ–‡ä»¶...")
    
    total_triples = 0
    successful = 0
    
    # å¤„ç†æ–‡ä»¶
    for i, (md_file, out_file) in enumerate(tqdm(files_to_process, desc="æ€»ä½“è¿›åº¦"), 1):
        try:
            print(f"\n{'='*60}")
            print(f"ğŸ” å¤„ç†ç¬¬ {i+start_idx}/{total_files} ä¸ªæ–‡ä»¶: {md_file.name}")
            
            count = process_single_file(md_file, out_file)
            successful += 1
            total_triples += count
            
            print(f"âœ… {md_file.name} â†’ æå– {count} ä¸ªä¸‰å…ƒç»„")
            print(f"ğŸ’¾ ä¿å­˜è‡³: {out_file}")
            
        except Exception as e:
            print(f"âŒ {md_file.name} å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\n{'='*60}")
    print(f"ğŸ‰ å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"   æˆåŠŸå¤„ç†: {successful}/{len(files_to_process)} ä¸ªæ–‡ä»¶")
    print(f"   æ€»å…±æå–: {total_triples} ä¸ªä¸‰å…ƒç»„")
    print(f"   å¹³å‡æ¯ä¸ªæ–‡ä»¶: {total_triples/max(1, successful):.1f} ä¸ªä¸‰å…ƒç»„")
    print(f"ğŸ“ ç»“æœä¿å­˜è‡³: {OUTPUT_ROOT}")

if __name__ == "__main__":
    main()